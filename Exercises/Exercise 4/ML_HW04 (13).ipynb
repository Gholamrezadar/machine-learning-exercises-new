{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 493,
   "metadata": {
    "id": "YBACGmh0brIr"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns; sns.set()\n",
    "\n",
    "import gym\n",
    "from amalearn.environment import EnvironmentBase\n",
    "from amalearn.reward import RewardBase\n",
    "from amalearn.agent import AgentBase\n",
    "\n",
    "sns.set_style(\"dark\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 494,
   "metadata": {
    "id": "pH6sNHxPbrIs"
   },
   "outputs": [],
   "source": [
    "class Environment(EnvironmentBase):\n",
    "    def __init__(self, obstacles = None ,id = 0, action_count=9, actionPrice=-1, goalReward=100, punish=-10, j_limit=15, i_limit=15, p=0.8, container=None):\n",
    "\n",
    "        # Define state_space and action_space required by the amalearn environment class\n",
    "        state_space = gym.spaces.MultiDiscrete([i_limit, j_limit])\n",
    "        action_space = gym.spaces.MultiDiscrete([3, 3])\n",
    "        super(Environment, self).__init__(action_space, state_space, id, container)\n",
    "\n",
    "        # Get the environment variables (:D)\n",
    "        self.action_price = actionPrice\n",
    "        self.goal_reward = goalReward\n",
    "        self.punish = punish\n",
    "        self.p = p\n",
    "        self.i_limit = i_limit\n",
    "        self.j_limit = j_limit\n",
    "\n",
    "        # Pass data between the environment and the agent\n",
    "        self.is_terminated = False\n",
    "        self.optimal_policy = None\n",
    "        self.optimal_V = None\n",
    "\n",
    "        # Agent's Position at the start\n",
    "        self.state = np.array([15,15])\n",
    "\n",
    "        # Agent's next position\n",
    "        self.state_p = None\n",
    "        \n",
    "        # Get Obstacle positions\n",
    "        self.obstacles = np.array([\n",
    "            # Bottom block\n",
    "            [12,6],\n",
    "            [13,6],\n",
    "            [14,6],\n",
    "            [15,6],\n",
    "            [12,7],\n",
    "            [13,7],\n",
    "            [14,7],\n",
    "            [15,7],\n",
    "\n",
    "            # Top block\n",
    "            [1,7],\n",
    "            [2,7],\n",
    "            [3,7],\n",
    "            [4,7],\n",
    "            [1,8],\n",
    "            [2,8],\n",
    "            [3,8],\n",
    "            [4,8],\n",
    "\n",
    "            # Right Block\n",
    "            [8,13],\n",
    "            [8,14],\n",
    "            [8,15],\n",
    "            [9,13],\n",
    "            [9,14],\n",
    "            [9,15]\n",
    "        ])\n",
    "\n",
    "        # Override if obstacles is provided\n",
    "        if obstacles is not None:\n",
    "            self.obstacles = obstacles\n",
    "\n",
    "    # Done 九九\n",
    "    def isStatePossible(self, state):\n",
    "        '''If the given state is possible (not out of the grid and not obstacle) return true'''\n",
    "\n",
    "        # Vertical Bound Check\n",
    "        if state[0] < 1 or state[0] > self.i_limit:\n",
    "            return False\n",
    "        \n",
    "        # Horizontal Bound Check\n",
    "        if state[1] < 1 or state[1] > self.j_limit:\n",
    "            return False\n",
    "        \n",
    "        # Obstacle Check(bitwise 'and' of first and second components' comparison)\n",
    "        if np.any((self.obstacles[:, 0] == state[0]) & (self.obstacles[:, 1] == state[1])):\n",
    "            return False\n",
    "        \n",
    "        return True\n",
    "    \n",
    "    # Done 九九\n",
    "    def isAccessible(self, state, state_p):\n",
    "        '''If the given state is accessible (we can reach state_p by doing an action from state) return true'''\n",
    "        # TODO: Optimization idea = norm_1(state_p - state)<1\n",
    "\n",
    "        for action in self.available_actions():\n",
    "            # print(state + action)\n",
    "            if np.all(np.array(state) + action == np.array(state_p)):\n",
    "                return True\n",
    "        return False\n",
    "\n",
    "    # TODO 游눝 九九\n",
    "    def getTransitionStatesAndProbs(self, state, action):\n",
    "        '''return the available states from 'state' by taking 'action' and their probabilities'''\n",
    "        accessible_states = []\n",
    "        probs = []\n",
    "        number_of_accessible_states = 0 #used to divide the probability of each slippage\n",
    "        action = np.array(action)\n",
    "        \n",
    "        # 1. The actual action:\n",
    "        state_p = np.array(state) + action\n",
    "        # print(\"next_state: \", state_p)\n",
    "        accessible_states.append(state_p)\n",
    "        probs.append(self.p) # 80% chance of going in the desired direction\n",
    "\n",
    "        # 2. Slipping actions:\n",
    "        # Loop over every action\n",
    "        for action_new in self.available_actions():\n",
    "            # Get the next state after taking the action 'action_new'\n",
    "            state_p = self.get_next_state(state, action_new)\n",
    "\n",
    "            # print(action_new, action, not np.all(action_new == action))\n",
    "            if not np.all(action_new == action):\n",
    "                number_of_accessible_states += 1\n",
    "                # TODO: Do we count obstacles? or just ignore them? if no then add a check here( check if accessible then increment the thing)\n",
    "                # 20% chance of going in the other directions\n",
    "                accessible_states.append(state_p)\n",
    "                probs.append(1.0 - self.p)\n",
    "        \n",
    "        # 3. Normalize the probabilities\n",
    "        # Apply divison on every item except the first one which is the correct action\n",
    "        probs = [probs[0]] + list(map(lambda x: x / number_of_accessible_states, probs[1:]))\n",
    "\n",
    "        return accessible_states, probs\n",
    "    \n",
    "    # Done 九九九\n",
    "    def getReward(self, state, state_p):\n",
    "        '''return reward of transition'''\n",
    "\n",
    "        # Reward for reaching the goal\n",
    "        if np.all(np.array(state_p) == np.array([1,1])):\n",
    "            return self.goal_reward\n",
    "\n",
    "        # Reward for state_p which is accessible from state\n",
    "        elif self.isAccessible(state, state_p) and self.isStatePossible(state_p):\n",
    "            return self.action_price\n",
    "\n",
    "        # Punishment for state_p which is not accessible from state\n",
    "        else:\n",
    "            return self.punish\n",
    "    \n",
    "    # TODO 游눖\n",
    "    def sample_all_rewards(self):\n",
    "        return \n",
    "    \n",
    "    # TODO 游눖\n",
    "    def calculate_reward(self, action):\n",
    "        return \n",
    "\n",
    "    # Done 九九\n",
    "    def terminated(self):\n",
    "        '''Returns true if the episode is terminated'''\n",
    "        if self.state == [1,1]:\n",
    "            return True\n",
    "        return False\n",
    "\n",
    "    # Done 九九\n",
    "    def observe(self):\n",
    "        '''Returns the current state'''\n",
    "        return self.state\n",
    "\n",
    "    # Done 九九\n",
    "    def available_actions(self):\n",
    "        '''returns a list of all actions.\n",
    "        0 : stay\n",
    "        1 : N\n",
    "        2 : NE\n",
    "        3 : E\n",
    "        4 : SE\n",
    "        5 : S\n",
    "        6 : SW\n",
    "        7 : W\n",
    "        8 : NW\n",
    "        '''\n",
    "        actions = np.array([\n",
    "            [0,0],   # stay\n",
    "            [0,1],   # N\n",
    "            [1,1],   # NE\n",
    "            [1,0],   # E\n",
    "            [1,-1],  # SE\n",
    "            [0,-1],  # S\n",
    "            [-1,-1], # SW\n",
    "            [-1,0],  # W\n",
    "            [-1,1]   # NW\n",
    "        ])\n",
    "\n",
    "        return actions\n",
    "\n",
    "    # Done 九九\n",
    "    def get_next_state(self, state, action):\n",
    "        '''Return the state after taking the action.\n",
    "        Return the current state if the resulting state is not valid.\n",
    "        '''\n",
    "\n",
    "        state_p = np.array(state) + np.array(action)\n",
    "        # print(f\"state_p: {state_p}\")\n",
    "        if self.isAccessible(state, state_p) and self.isStatePossible(state_p):\n",
    "            return np.array(state_p)\n",
    "        else:\n",
    "            return np.array(state)\n",
    "    \n",
    "    def next_state(self, action):\n",
    "        '''Return the state after taking the action.\n",
    "        Return the current state if the resulting state is not valid.\n",
    "        '''\n",
    "\n",
    "        state_p = np.array(self.state) + np.array(action)\n",
    "        if self.isAccessible(self.state, state_p) and self.isStatePossible(state_p):\n",
    "            return np.array(state_p)\n",
    "        else:\n",
    "            return np.array(self.state)\n",
    "\n",
    "    # Done 九\n",
    "    def reset(self):\n",
    "        '''Reset the environment to the initial state'''\n",
    "        self.state = np.array([15,15])\n",
    "\n",
    "    # TODO 游눖\n",
    "    def render(self, mode='human'):\n",
    "        #print('{}:\\taction={}'.format(self.state['length'], self.state['last_action']))\n",
    "        return \n",
    "\n",
    "    # TODO 游눖\n",
    "    def close(self):\n",
    "        return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 495,
   "metadata": {
    "id": "898Jlhsycyes"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class Agent(AgentBase):\n",
    "    def __init__(self, id=0, environment=None, discount=0.9, theta=0.1, algorithm='policy_iteration'):\n",
    "        \n",
    "        super(Agent, self).__init__(id, environment)\n",
    "\n",
    "        self.environment = environment\n",
    "        self.discount = discount\n",
    "        self.theta = theta\n",
    "        self.algorithm = algorithm\n",
    "\n",
    "        # mapp states to its ids TODO: Unneccessary?\n",
    "        self.mapp = {}\n",
    "\n",
    "        m,n = self.environment.i_limit, self.environment.j_limit\n",
    "\n",
    "        # Initialize V with zeros\n",
    "        self.V = np.zeros((m,n))\n",
    "\n",
    "        # Initialize policy with random actions\n",
    "        random_indices = np.random.randint(self.environment.available_actions().shape[0], size=m*n)\n",
    "        self.policy = self.environment.available_actions()[random_indices].reshape((m, n, 2))\n",
    "        # print(self.policy.shape)\n",
    "\n",
    "    # Done 九\n",
    "    def sum_states(self, state, action, debug=False):\n",
    "        '''The sigma line in the policy iteration algorithm.\n",
    "        sums the reward of every accessible state' from current state\n",
    "        via action 'action'.\n",
    "        '''\n",
    "        \n",
    "        # Which states can we go from here via action 'action' and what are their probabilities? (Slipping)\n",
    "        accessible_states, probs = self.environment.getTransitionStatesAndProbs(state, action)\n",
    "\n",
    "        if debug:\n",
    "            print(\"# probs\")\n",
    "            print(np.array(accessible_states))\n",
    "            print(probs)\n",
    "        \n",
    "        # The Sigma\n",
    "        summ = 0\n",
    "        for i in range(len(accessible_states)):\n",
    "            # Reward\n",
    "            r = self.environment.getReward(state, accessible_states[i])\n",
    "            # -1 because V is a 0-based numpy array but our world is 1-based\n",
    "            summ += (r + self.discount * self.V[state[0]-1, state[1]-1]) * probs[i]\n",
    "        \n",
    "        return summ\n",
    "\n",
    "        \n",
    "    # Done 九\n",
    "    def policy_evaluation(self):\n",
    "        '''Policy Evaluation step.\n",
    "        Iterates until 풊V is less than 'theta'.\n",
    "        '''\n",
    "        import time\n",
    "\n",
    "        counter = 1\n",
    "        # Iterate until Convergence\n",
    "        while True:\n",
    "            delta = 0\n",
    "            start_time = time.perf_counter_ns()\n",
    "            # For every state s, calculate V[s]\n",
    "            for i in range(self.environment.i_limit):\n",
    "                for j in range(self.environment.j_limit):\n",
    "                    old_v = self.V[i,j]\n",
    "                    # +1 because our world is 1-based but i,j are 0-based\n",
    "                    state = [i+1,j+1]\n",
    "                    # Get the current action\n",
    "                    action = self.policy[i,j]\n",
    "                    # Calculate the new value\n",
    "                    self.V[i,j] = self.sum_states(state, action)\n",
    "                    # Calculate the difference\n",
    "                    delta = max(delta, abs(self.V[i,j] - old_v))\n",
    "            \n",
    "\n",
    "            # print(f\"Iteration: {counter} done in {(time.perf_counter_ns() - start_time)/1000000:.3f} ms delta:{delta}\")\n",
    "            counter += 1\n",
    "\n",
    "            if delta < self.theta:\n",
    "                break\n",
    "                \n",
    "    # Done 九\n",
    "    def policy_improvement(self):\n",
    "        '''Policy Improvment step.\n",
    "        Updates the policy based on the V that we get from 'policy_evaluation' step.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        is_policy_stable: bool\n",
    "            True if the policy is stable, False otherwise.\n",
    "        '''\n",
    "        is_policy_stable = True\n",
    "\n",
    "        # For every state s, calculate [s] based on V[s]\n",
    "        for i in range(self.environment.i_limit):\n",
    "            for j in range(self.environment.j_limit):\n",
    "                old_action = self.policy[i,j]\n",
    "                state = [i+1,j+1]\n",
    "\n",
    "                # Loop over all actions and find the best one\n",
    "                best_action = old_action.copy()\n",
    "                for action in self.environment.available_actions():\n",
    "                    # print(self.sum_states(state, action))\n",
    "                    if self.sum_states(state, action) > self.sum_states(state, best_action):\n",
    "                        best_action = action.copy()\n",
    "                \n",
    "                # print(old_action, best_action)\n",
    "                # print()\n",
    "                # Check for stability of policy\n",
    "                if np.all(best_action != old_action):\n",
    "                    is_policy_stable = False\n",
    "                \n",
    "        return is_policy_stable\n",
    "    \n",
    "    # TODO 游눖\n",
    "    def policy_iteration(self):\n",
    "        # in a loop\n",
    "        is_policy_stable = False\n",
    "        while not is_policy_stable:\n",
    "            self.policy_evaluation()\n",
    "            is_policy_stable = self.policy_improvement()\n",
    "    \n",
    "    # TODO 游눖\n",
    "    def value_iteration(self):\n",
    "        pass\n",
    "    \n",
    "    # TODO 游눖\n",
    "    def take_action(self) -> tuple[object, float, bool, object]:\n",
    "        # One step of policy iteration\n",
    "        self.policy_evaluation()\n",
    "        is_policy_stable = self.policy_improvement()\n",
    "        \n",
    "        # Pass data from agent to environment\n",
    "        self.environment.optimal_policy = self.policy\n",
    "        self.environment.optimal_value = self.V\n",
    "        self.environment.is_terminated = is_policy_stable\n",
    "\n",
    "        # Step method of the environment\n",
    "        observation, reward, done, info = self.environment.step(action)\n",
    "        \n",
    "        # Render the environment after every action\n",
    "        # self.environment.render()\n",
    "\n",
    "        return observation, reward, done, info\n",
    "\n",
    "    def test(self):\n",
    "        is_policy_stable = False\n",
    "        while not is_policy_stable:\n",
    "            self.policy_evaluation()\n",
    "            is_policy_stable = self.policy_improvement()\n",
    "            print(is_policy_stable)\n",
    "        \n",
    "        return self.V, self.policy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main (Test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 496,
   "metadata": {},
   "outputs": [],
   "source": [
    "environment = Environment()\n",
    "agent = Agent(environment=environment, theta=80)\n",
    "# agent.take_action()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 497,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([array([2, 0]),\n",
       "  array([2, 1]),\n",
       "  array([2, 2]),\n",
       "  array([3, 2]),\n",
       "  array([3, 1]),\n",
       "  array([2, 1]),\n",
       "  array([2, 1]),\n",
       "  array([1, 1]),\n",
       "  array([1, 2])],\n",
       " [0.8,\n",
       "  0.024999999999999994,\n",
       "  0.024999999999999994,\n",
       "  0.024999999999999994,\n",
       "  0.024999999999999994,\n",
       "  0.024999999999999994,\n",
       "  0.024999999999999994,\n",
       "  0.024999999999999994,\n",
       "  0.024999999999999994])"
      ]
     },
     "execution_count": 497,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "environment.getTransitionStatesAndProbs([2,1], [0,-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 513,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# probs\n",
      "[[16 16]\n",
      " [15 15]\n",
      " [15 15]\n",
      " [15 15]\n",
      " [15 15]\n",
      " [15 14]\n",
      " [14 14]\n",
      " [14 15]\n",
      " [15 15]]\n",
      "[0.8, 0.024999999999999994, 0.024999999999999994, 0.024999999999999994, 0.024999999999999994, 0.024999999999999994, 0.024999999999999994, 0.024999999999999994, 0.024999999999999994]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "-8.200000000000003"
      ]
     },
     "execution_count": 513,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent.sum_states([15,15], [1,1], debug=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 499,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# probs\n",
      "[[-1  9]\n",
      " [ 0  9]\n",
      " [ 0  9]\n",
      " [ 1 10]\n",
      " [ 1  9]\n",
      " [ 0  9]\n",
      " [ 0  9]\n",
      " [ 0  9]\n",
      " [ 0  9]]\n",
      "[0.8, 0.024999999999999994, 0.024999999999999994, 0.024999999999999994, 0.024999999999999994, 0.024999999999999994, 0.024999999999999994, 0.024999999999999994, 0.024999999999999994]\n",
      "-9.55\n",
      "# probs\n",
      "[[ 0  9]\n",
      " [ 1  9]\n",
      " [ 1 10]\n",
      " [ 2 10]\n",
      " [ 2  9]\n",
      " [ 1  9]\n",
      " [ 1  9]\n",
      " [ 1  9]\n",
      " [ 1  9]]\n",
      "[0.8, 0.024999999999999994, 0.024999999999999994, 0.024999999999999994, 0.024999999999999994, 0.024999999999999994, 0.024999999999999994, 0.024999999999999994, 0.024999999999999994]\n",
      "-8.200000000000003\n",
      "# probs\n",
      "[[ 1  9]\n",
      " [ 2  9]\n",
      " [ 2 10]\n",
      " [ 3 10]\n",
      " [ 3  9]\n",
      " [ 2  9]\n",
      " [ 2  9]\n",
      " [ 2  9]\n",
      " [ 1 10]]\n",
      "[0.8, 0.024999999999999994, 0.024999999999999994, 0.024999999999999994, 0.024999999999999994, 0.024999999999999994, 0.024999999999999994, 0.024999999999999994, 0.024999999999999994]\n",
      "-1.0000000000000002\n",
      "# probs\n",
      "[[ 2  9]\n",
      " [ 3  9]\n",
      " [ 3 10]\n",
      " [ 4 10]\n",
      " [ 4  9]\n",
      " [ 3  9]\n",
      " [ 3  9]\n",
      " [ 3  9]\n",
      " [ 2 10]]\n",
      "[0.8, 0.024999999999999994, 0.024999999999999994, 0.024999999999999994, 0.024999999999999994, 0.024999999999999994, 0.024999999999999994, 0.024999999999999994, 0.024999999999999994]\n",
      "-1.0000000000000002\n",
      "# probs\n",
      "[[ 3  9]\n",
      " [ 4  9]\n",
      " [ 4 10]\n",
      " [ 5 10]\n",
      " [ 5  9]\n",
      " [ 5  8]\n",
      " [ 4  9]\n",
      " [ 4  9]\n",
      " [ 3 10]]\n",
      "[0.8, 0.024999999999999994, 0.024999999999999994, 0.024999999999999994, 0.024999999999999994, 0.024999999999999994, 0.024999999999999994, 0.024999999999999994, 0.024999999999999994]\n",
      "-1.0000000000000002\n",
      "# probs\n",
      "[[ 4  9]\n",
      " [ 5  9]\n",
      " [ 5 10]\n",
      " [ 6 10]\n",
      " [ 6  9]\n",
      " [ 6  8]\n",
      " [ 5  8]\n",
      " [ 5  9]\n",
      " [ 4 10]]\n",
      "[0.8, 0.024999999999999994, 0.024999999999999994, 0.024999999999999994, 0.024999999999999994, 0.024999999999999994, 0.024999999999999994, 0.024999999999999994, 0.024999999999999994]\n",
      "-1.0000000000000002\n",
      "# probs\n",
      "[[ 5  9]\n",
      " [ 6  9]\n",
      " [ 6 10]\n",
      " [ 7 10]\n",
      " [ 7  9]\n",
      " [ 7  8]\n",
      " [ 6  8]\n",
      " [ 5  8]\n",
      " [ 5 10]]\n",
      "[0.8, 0.024999999999999994, 0.024999999999999994, 0.024999999999999994, 0.024999999999999994, 0.024999999999999994, 0.024999999999999994, 0.024999999999999994, 0.024999999999999994]\n",
      "-1.0000000000000002\n",
      "# probs\n",
      "[[ 6  9]\n",
      " [ 7  9]\n",
      " [ 7 10]\n",
      " [ 8 10]\n",
      " [ 8  9]\n",
      " [ 8  8]\n",
      " [ 7  8]\n",
      " [ 6  8]\n",
      " [ 6 10]]\n",
      "[0.8, 0.024999999999999994, 0.024999999999999994, 0.024999999999999994, 0.024999999999999994, 0.024999999999999994, 0.024999999999999994, 0.024999999999999994, 0.024999999999999994]\n",
      "-1.0000000000000002\n",
      "# probs\n",
      "[[ 7  9]\n",
      " [ 8  9]\n",
      " [ 8 10]\n",
      " [ 9 10]\n",
      " [ 9  9]\n",
      " [ 9  8]\n",
      " [ 8  8]\n",
      " [ 7  8]\n",
      " [ 7 10]]\n",
      "[0.8, 0.024999999999999994, 0.024999999999999994, 0.024999999999999994, 0.024999999999999994, 0.024999999999999994, 0.024999999999999994, 0.024999999999999994, 0.024999999999999994]\n",
      "-1.0000000000000002\n",
      "# probs\n",
      "[[ 8  9]\n",
      " [ 9  9]\n",
      " [ 9 10]\n",
      " [10 10]\n",
      " [10  9]\n",
      " [10  8]\n",
      " [ 9  8]\n",
      " [ 8  8]\n",
      " [ 8 10]]\n",
      "[0.8, 0.024999999999999994, 0.024999999999999994, 0.024999999999999994, 0.024999999999999994, 0.024999999999999994, 0.024999999999999994, 0.024999999999999994, 0.024999999999999994]\n",
      "-1.0000000000000002\n",
      "# probs\n",
      "[[ 9  9]\n",
      " [10  9]\n",
      " [10 10]\n",
      " [11 10]\n",
      " [11  9]\n",
      " [11  8]\n",
      " [10  8]\n",
      " [ 9  8]\n",
      " [ 9 10]]\n",
      "[0.8, 0.024999999999999994, 0.024999999999999994, 0.024999999999999994, 0.024999999999999994, 0.024999999999999994, 0.024999999999999994, 0.024999999999999994, 0.024999999999999994]\n",
      "-1.0000000000000002\n",
      "# probs\n",
      "[[10  9]\n",
      " [11  9]\n",
      " [11 10]\n",
      " [12 10]\n",
      " [12  9]\n",
      " [12  8]\n",
      " [11  8]\n",
      " [10  8]\n",
      " [10 10]]\n",
      "[0.8, 0.024999999999999994, 0.024999999999999994, 0.024999999999999994, 0.024999999999999994, 0.024999999999999994, 0.024999999999999994, 0.024999999999999994, 0.024999999999999994]\n",
      "-1.0000000000000002\n",
      "# probs\n",
      "[[11  9]\n",
      " [12  9]\n",
      " [12 10]\n",
      " [13 10]\n",
      " [13  9]\n",
      " [13  8]\n",
      " [12  8]\n",
      " [11  8]\n",
      " [11 10]]\n",
      "[0.8, 0.024999999999999994, 0.024999999999999994, 0.024999999999999994, 0.024999999999999994, 0.024999999999999994, 0.024999999999999994, 0.024999999999999994, 0.024999999999999994]\n",
      "-1.0000000000000002\n",
      "# probs\n",
      "[[12  9]\n",
      " [13  9]\n",
      " [13 10]\n",
      " [14 10]\n",
      " [14  9]\n",
      " [14  8]\n",
      " [13  8]\n",
      " [12  8]\n",
      " [12 10]]\n",
      "[0.8, 0.024999999999999994, 0.024999999999999994, 0.024999999999999994, 0.024999999999999994, 0.024999999999999994, 0.024999999999999994, 0.024999999999999994, 0.024999999999999994]\n",
      "-1.0000000000000002\n",
      "# probs\n",
      "[[13  9]\n",
      " [14  9]\n",
      " [14 10]\n",
      " [15 10]\n",
      " [15  9]\n",
      " [15  8]\n",
      " [14  8]\n",
      " [13  8]\n",
      " [13 10]]\n",
      "[0.8, 0.024999999999999994, 0.024999999999999994, 0.024999999999999994, 0.024999999999999994, 0.024999999999999994, 0.024999999999999994, 0.024999999999999994, 0.024999999999999994]\n",
      "-1.0000000000000002\n"
     ]
    }
   ],
   "source": [
    "for i in range(0,15):\n",
    "    print(agent.sum_states(np.array([i,9]), np.array([-1,0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 500,
   "metadata": {},
   "outputs": [],
   "source": [
    "# agent.sum_states([15,15], [1,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 501,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 501,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "environment.getReward([15,15], [16,16])\n",
    "environment.getReward([15,15], [1,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 502,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([14, 14])"
      ]
     },
     "execution_count": 502,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "environment.get_next_state([15,15], [-1,-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 503,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 503,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "environment.isStatePossible([14,15])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 504,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 504,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "environment.isAccessible([15,15], [14,15])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 505,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([array([14, 15]),\n",
       "  array([15, 15]),\n",
       "  array([15, 15]),\n",
       "  array([15, 15]),\n",
       "  array([15, 15]),\n",
       "  array([15, 15]),\n",
       "  array([15, 14]),\n",
       "  array([14, 14]),\n",
       "  array([15, 15])],\n",
       " [0.8,\n",
       "  0.024999999999999994,\n",
       "  0.024999999999999994,\n",
       "  0.024999999999999994,\n",
       "  0.024999999999999994,\n",
       "  0.024999999999999994,\n",
       "  0.024999999999999994,\n",
       "  0.024999999999999994,\n",
       "  0.024999999999999994])"
      ]
     },
     "execution_count": 505,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "environment.getTransitionStatesAndProbs([15,15], [-1,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 506,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 506,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "environment.isAccessible([15,15], [14,1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 507,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 0 ns\n",
      "Wall time: 0 ns\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# a,b = agent.test();"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "env.ipynb",
   "provenance": []
  },
  "interpreter": {
   "hash": "83fad98a7911d3a2a55c2e5234aea09e74d0252d0d10d90172c6e09f21426bdf"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
