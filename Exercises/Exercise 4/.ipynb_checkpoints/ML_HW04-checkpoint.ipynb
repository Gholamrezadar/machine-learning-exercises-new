{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mBMoPLmGbrIn"
      },
      "outputs": [],
      "source": [
        "from amalearn.reward import RewardBase\n",
        "from amalearn.agent import AgentBase"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YBACGmh0brIr"
      },
      "outputs": [],
      "source": [
        "from amalearn.environment import EnvironmentBase\n",
        "import gym\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pH6sNHxPbrIs"
      },
      "outputs": [],
      "source": [
        "\n",
        "class Environment(EnvironmentBase):\n",
        "    def __init__(self, obstacle = [] ,id = 0, action_count=9, actionPrice = -1, goalReward = 100, punish=-10, j_limit = 10, i_limit = 10, p = 0.8, container=None):\n",
        "        \"\"\"\n",
        "        initialize your variables\n",
        "        \"\"\"\n",
        "        state_space = gym.spaces.MultiDiscrete([17,17])\n",
        "        action_space = gym.spaces.Discrete(9)\n",
        "        super(Environment, self).__init__(action_space,state_space,id,container)\n",
        "        \n",
        "        \n",
        "        self.opt_policy = None\n",
        "        self.opt_values = None\n",
        "        self.state = [15,15]\n",
        "        self.mu = p\n",
        "        self.goalReward = goalReward\n",
        "        self.punish = punish\n",
        "        self.actionPrice = actionPrice\n",
        "        self.state_space = state_space\n",
        "        \n",
        "    def isStatePossible(self, state):\n",
        "        \"\"\"if given state is possible (not out of the grid and not obstacle) return ture\"\"\"\n",
        "        x, y = state[0], state[1]\n",
        "        if x < 1 or x > 15 or y < 1 or y > 15:\n",
        "            return False\n",
        "        \n",
        "        if x == 6 or x == 7:\n",
        "            if y > 11:\n",
        "                return False\n",
        "        \n",
        "        if x == 7 or x == 8:\n",
        "            if y < 5: \n",
        "                return False\n",
        "            \n",
        "        if x > 12:\n",
        "            if y == 8 or y == 9:\n",
        "                return False\n",
        "            \n",
        "        return True\n",
        "    \n",
        "    def isAccessible(self, state, state_p):\n",
        "        \"\"\"if given state is Accesible (we can reach state_p by doing an action from state) return true\"\"\"\n",
        "        if np.abs(state[0] - state_p[0]) > 1 or np.abs(state[1] - state_p[1]) > 1:\n",
        "            return False\n",
        "        else:\n",
        "            return self.isStatePossible(state_p)\n",
        "        \n",
        "    def getTransitionStatesAndProbs(self, state, action, state_p):\n",
        "        \"\"\"return probability of transition or T(sp,a,s)\"\"\"\n",
        "        act = self.get_action(action)\n",
        "        p = 0\n",
        "        f = 0\n",
        "        if state[0] + act[0] == state_p[0] and state[1] + act[1] == state_p[1]:\n",
        "            p = p + self.mu\n",
        "        \n",
        "        for i in [-1,0,1]:\n",
        "            for j in [-1,0,1]:\n",
        "                f += self.isAccessible(state, [state[0]+i, state[1]+j])\n",
        "        \n",
        "        if self.isAccessible(state, state_p):\n",
        "            p = p + 0.2/f\n",
        "        \n",
        "        return p\n",
        "    \n",
        "    def getReward(self, state, action, state_p):\n",
        "        \"\"\"return reward of transition\"\"\"\n",
        "        if state_p[0] == 1 and state_p[1] == 1:\n",
        "            return self.goalReward\n",
        "        \n",
        "        if self.isAccessible(state, state_p):\n",
        "            return self.actionPrice\n",
        "        \n",
        "        return self.punish\n",
        "    \n",
        "    def sample_all_rewards(self):\n",
        "        return \n",
        "    \n",
        "    def get_action(self, action):\n",
        "        if action == 0:\n",
        "            return [-1,-1]\n",
        "        if action == 1:\n",
        "            return [-1,0]\n",
        "        if action == 2:\n",
        "            return [-1,1]\n",
        "        if action == 3:\n",
        "            return [0,-1]\n",
        "        if action == 4:\n",
        "            return [0,0]\n",
        "        if action == 5:\n",
        "            return [0,1]\n",
        "        if action == 6:\n",
        "            return [1,-1]\n",
        "        if action == 7:\n",
        "            return [1,0]\n",
        "        if action == 8:\n",
        "            return [1,1]\n",
        "\n",
        "\n",
        "    def calculate_reward(self, action):\n",
        "        s = self.state\n",
        "        sp = self.pred(action)\n",
        "        return self.getReward(s,action,sp)\n",
        "\n",
        "    def terminated(self):\n",
        "        return self.state == [1,1] \n",
        "\n",
        "    def observe(self):\n",
        "        return self.state\n",
        "\n",
        "    def available_actions(self):\n",
        "        return [0,1,2,3,4,5,6,7,8]\n",
        "\n",
        "    def pred(self, action):\n",
        "        s = self.state\n",
        "        prob = np.zeros((9,))\n",
        "        act = self.get_action(action)\n",
        "        s_p = []\n",
        "        for i in [-1,0,1]:\n",
        "            for j in [-1,0,1]:\n",
        "                s_p.append([s[0] + i, s[1] + j])\n",
        "        for i in range(9):\n",
        "            prob[i] = self.getTransitionStatesAndProbs(s, action, s_p[i])\n",
        "            \n",
        "        c = np.random.choice(self.available_actions(), p = prob)\n",
        "        c = self.get_action(c)\n",
        "        next_state = [0,0]\n",
        "        next_state[0] = s[0] + c[0]\n",
        "        next_state[1] = s[1] + c[1]\n",
        "        self.sp = next_state\n",
        "        return next_state\n",
        "    \n",
        "    def next_state(self,action):\n",
        "        s = self.state\n",
        "        if self.isAccessible(s,self.sp):\n",
        "            self.state = self.sp\n",
        "            return self.sp\n",
        "        return s\n",
        "    \n",
        "    def reset(self):\n",
        "        self.state = [15,15]\n",
        "\n",
        "    def render(self, mode='human'):\n",
        "        #print('{}:\\taction={}'.format(self.state['length'], self.state['last_action']))\n",
        "        return \n",
        "\n",
        "    def close(self):\n",
        "        return"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "898Jlhsycyes"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "class Agent(AgentBase):\n",
        "    def __init__(self, id, environment, discount, theta, policy_iteration = True):\n",
        "        #initialize a random policy and V(s) = 0 for each state\n",
        "        self.environment = environment\n",
        "        self.mapp = {}\n",
        "        #mapp states to its ids\n",
        "        self.V = np.zeros((17,17))\n",
        "        #init V\n",
        "        self.policy = np.random.randint(0,9,(17,17))\n",
        "        #init policy\n",
        "        super(Agent, self).__init__(id, environment)\n",
        "        self.discount = discount\n",
        "        self.theta = theta\n",
        "        self.policy_iteration = policy_iteration\n",
        "    def policy_evaluation(self):\n",
        "        V = self.V\n",
        "        while True:\n",
        "            delta = 0.01\n",
        "            for i in range(1,16):\n",
        "                for j in range(1,16):\n",
        "                    v = self.V[i,j]\n",
        "                    v_new = 0\n",
        "                    action = self.policy[i,j]\n",
        "#                     prob = np.zeros((9,))\n",
        "#                     rew = np.zeros((9,))\n",
        "                    for k in [-1,0,1]:\n",
        "                        for l in [-1,0,1]:\n",
        "                            rew = ( self.environment.getReward([i,j], action, [i + k, j + l]))\n",
        "                            prob = (self.environment.getTransitionStatesAndProbs([i,j], action, [i + k, j + l]))\n",
        "                            v_new += prob*(rew + self.discount*self.V[i+k,j+l])\n",
        "                    \n",
        "                    V[i,j] = v_new\n",
        "                    delta = np.max([delta, np.abs(v_new - v)])\n",
        "                    if delta < self.theta:\n",
        "                        return V\n",
        "                    \n",
        "    def policy_improvement(self):\n",
        "        stable = True\n",
        "        for i in range(1,16):\n",
        "            for j in range(1,16):\n",
        "                old_action = self.policy[i,j]\n",
        "                qs = []\n",
        "                for action in range(9):\n",
        "#                     prob = np.zeros((9,))\n",
        "#                     rew = np.zeros((9,))\n",
        "                    q = 0\n",
        "                    for k in [-1,0,1]:\n",
        "                        for l in [-1,0,1]:\n",
        "                            rew = ( self.environment.getReward([i,j], action, [i + k, j + l]))\n",
        "                            prob = (self.environment.getTransitionStatesAndProbs([i,j], action, [i + k, j + l]))\n",
        "                            q += prob*(rew + self.discount*self.V[i+k,j+l])\n",
        "\n",
        "                    qs.append(q)\n",
        "\n",
        "                best_action = np.argmax(qs)\n",
        "                self.policy[i,j] = best_action\n",
        "                if best_action != old_action:\n",
        "                    stable = False\n",
        "        return stable\n",
        "    \n",
        "    def policy_iter(self):\n",
        "        values = self.V\n",
        "        policy = self.policy\n",
        "        stable = False\n",
        "        while not stable:\n",
        "            self.V = self.policy_evaluation()\n",
        "            stable = self.policy_improvement()\n",
        "            \n",
        "    def value_iteration(self):\n",
        "        V = self.V\n",
        "        pol = np.zeros((17,17), dtype = np.int)\n",
        "        delta = np.float(0)\n",
        "        while True:\n",
        "            V2 = V\n",
        "            for i in range(1,16):\n",
        "                for j in range(1,16):\n",
        "                    v = self.V[i,j]\n",
        "                    Vs = []\n",
        "                    for action in range(9):\n",
        "                        value = 0\n",
        "                        for k in [-1,0,1]:\n",
        "                            for l in [-1,0,1]:\n",
        "                                rew = ( self.environment.getReward([i,j], action, [i + k, j + l]))\n",
        "                                prob = (self.environment.getTransitionStatesAndProbs([i,j], action, [i + k, j + l]))\n",
        "                                value += prob*(rew + self.discount*self.V[i+k,j+l])\n",
        "\n",
        "                        Vs.append(value)\n",
        "                        V2[i,j] = np.max(Vs)\n",
        "                        best_action = int(np.argmax(Vs))\n",
        "                        pol[i,j] = best_action\n",
        "                        delta = np.max([delta, abs(V[i,j] - V2[i,j])])\n",
        "            if delta < self.theta:\n",
        "                self.policy = pol\n",
        "                break\n",
        "            V = V2\n",
        "    \n",
        "    def visualize_policy(self):\n",
        "        fig, ax = plt.subplots()\n",
        "\n",
        "        ax.matshow(self.policy[1:16,1:16].T, cmap='summer')\n",
        "\n",
        "        for i in range(15):\n",
        "            for j in range(15):\n",
        "                c = self.policy.T[j+1,i+1]\n",
        "                ax.text(i, j, str(c), va='center', ha='center')\n",
        "        plt.axis('off')\n",
        "        plt.show() \n",
        "        \n",
        "    def take_action(self):\n",
        "#         self.environment.reset()\n",
        "        steps = 0\n",
        "        if self.policy_iteration:\n",
        "            self.policy_iter()\n",
        "        else:\n",
        "            self.value_iteration()\n",
        "        states = []\n",
        "        actions = []\n",
        "        rews = []\n",
        "        while True:\n",
        "            states.append(self.environment.state)\n",
        "            state = states[-1]\n",
        "            action = self.policy[state[0], state[1]]\n",
        "            actions.append(action)\n",
        "            a,rew,b,c = self.environment.step(action)\n",
        "            rews.append(rew)\n",
        "            if self.environment.terminated():\n",
        "                states.append(self.environment.state)\n",
        "                break\n",
        "        return states, actions, rews"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rnAslq_YhJ2Z"
      },
      "outputs": [],
      "source": [
        "ac = np.array([0,1,2,3,4,5,6,7,8]).reshape((3,3))\n",
        "fig, ax = plt.subplots()\n",
        "\n",
        "ax.matshow(ac, cmap='summer')\n",
        "\n",
        "for i in range(3):\n",
        "    for j in range(3):\n",
        "        c = ac[i,j]\n",
        "        ax.text(i, j, str(c), va='center', ha='center')\n",
        "\n",
        "plt.axis('off')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f9yIPD3ChJ2b"
      },
      "outputs": [],
      "source": [
        "### Base case\n",
        "\n",
        "env = Environment(punish = -1, actionPrice = -0.01, goalReward= 1000)\n",
        "agent1 = Agent(0, env, 0.9, 0.1)\n",
        "agent1.visualize_policy()\n",
        "states, actions, rews = agent1.take_action()\n",
        "agent1.visualize_policy()\n",
        "plt.plot(np.array(states)[:,0], np.array(states)[:,1])\n",
        "plt.grid()\n",
        "plt.gca().invert_yaxis()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OigC49dGhJ2c"
      },
      "outputs": [],
      "source": [
        "np.array(states)[0,:]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i_YpwoKjhJ2d"
      },
      "outputs": [],
      "source": [
        "### no punishment\n",
        "\n",
        "env = Environment(punish = -0.01, actionPrice=0, goalReward=1000)\n",
        "agent2 = Agent(0, env, 0.9, 0.1)\n",
        "agent2.visualize_policy()\n",
        "states, actions, rews = agent2.take_action()\n",
        "agent2.visualize_policy()\n",
        "plt.plot(np.array(states)[:,0], np.array(states)[:,1])\n",
        "# plt.axis('off')\n",
        "plt.grid()\n",
        "plt.gca().invert_yaxis()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XCBO1aOehJ2e"
      },
      "outputs": [],
      "source": [
        "### high punishment\n",
        "env = Environment(punish = -10, actionPrice=-1, goalReward=100)\n",
        "agent = Agent(0, env, 0.9, 0.1)\n",
        "agent.visualize_policy()\n",
        "states,actions,rewards = agent.take_action()\n",
        "agent.visualize_policy()\n",
        "plt.plot(np.array(states)[:,0], np.array(states)[:,1])\n",
        "# plt.axis('off')\n",
        "plt.grid()\n",
        "plt.gca().invert_yaxis()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "mmDgdZT7hJ2f"
      },
      "outputs": [],
      "source": [
        "n_steps_punish = []\n",
        "for i in range(10):\n",
        "    env = Environment(punish = -1, actionPrice=-0.01, goalReward=1000)\n",
        "    agent = Agent(0, env, 0.9, 0.1)\n",
        "    states,actions,rewards = agent.take_action()\n",
        "    n_steps_punish.append(len(states))\n",
        "    print(i, len(states))\n",
        "    \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V6cSNemUhJ2g"
      },
      "outputs": [],
      "source": [
        "n_steps_no_punish = []\n",
        "for i in range(10):\n",
        "    env = Environment(punish = -0.01, actionPrice=0, goalReward=1000)\n",
        "    agent = Agent(0, env, 0.9, 0.1)\n",
        "    states,actions,rewards = agent.take_action()\n",
        "    n_steps_no_punish.append(len(states))\n",
        "    print(i, len(states))\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ePROvIxyhJ2h"
      },
      "outputs": [],
      "source": [
        "n_steps_high_punish = []\n",
        "for i in range(10):\n",
        "    env = Environment(punish = -10, actionPrice=-1, goalReward=100)\n",
        "    agent = Agent(0, env, 0.9, 0.1)\n",
        "    states,actions,rewards = agent.take_action()\n",
        "    n_steps_high_punish.append(len(states))\n",
        "    print(i, len(states))\n",
        "    \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g8SXGcohhJ2h"
      },
      "outputs": [],
      "source": [
        "\n",
        "print(f\"\"\"\n",
        "\n",
        "average number of steps for base case: {np.mean(n_steps_punish)}\n",
        "\n",
        "average number of steps for no punishment case: {np.mean(n_steps_no_punish)}\n",
        "\n",
        "average number of steps for high punishment case: {np.mean(n_steps_high_punish)}\n",
        "\n",
        "\"\"\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "3o-mcQJHhJ2i"
      },
      "outputs": [],
      "source": [
        "reward1 = []\n",
        "for i in range(10):\n",
        "    env = Environment(punish = -1, actionPrice=-0.01, goalReward=1000)\n",
        "    agent = Agent(0, env, 0, 0.1)\n",
        "    states,actions,rewards = agent.take_action()\n",
        "    reward1.append(np.sum(rewards))\n",
        "    print(i, np.sum(rewards))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "aBrzDEm_hJ2i"
      },
      "outputs": [],
      "source": [
        "reward2 = []\n",
        "for i in range(10):\n",
        "    env = Environment(punish = -1, actionPrice=-0.01, goalReward=1000)\n",
        "    agent = Agent(0, env, 0.3, 0.1)\n",
        "    states,actions,rewards = agent.take_action()\n",
        "    reward2.append(np.sum(rewards))\n",
        "    print(i, np.sum(rewards))\n",
        "    \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "bzsLn7KRhJ2j"
      },
      "outputs": [],
      "source": [
        "    \n",
        "reward3 = []\n",
        "for i in range(10):\n",
        "    env = Environment(punish = -1, actionPrice=-0.01, goalReward=1000)\n",
        "    agent = Agent(0, env, 0.5, 0.1)\n",
        "    states,actions,rewards = agent.take_action()\n",
        "    reward3.append(np.sum(rewards))\n",
        "    print(i, np.sum(rewards))\n",
        "    \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "p0AAUTGFhJ2j"
      },
      "outputs": [],
      "source": [
        "    \n",
        "reward4 = []\n",
        "for i in range(10):\n",
        "    env = Environment(punish = -1, actionPrice=-0.01, goalReward=1000)\n",
        "    agent = Agent(0, env, 0.8, 0.1)\n",
        "    states,actions,rewards = agent.take_action()\n",
        "    reward4.append(np.sum(rewards))\n",
        "    print(i, np.sum(rewards))\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g0MqQvaEhJ2k"
      },
      "outputs": [],
      "source": [
        "\n",
        "print(f\"\"\"\n",
        "discount factor = 0, mean reward = {np.mean(reward1)}\n",
        "discount factor = 0.3, mean reward = {np.mean(reward2)}\n",
        "discount factor = 0.5, mean reward = {np.mean(reward3)}\n",
        "discount factor = 0.8, mean reward = {np.mean(reward4)}\n",
        "\n",
        "\"\"\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UDNTYQ8uhJ2k"
      },
      "outputs": [],
      "source": [
        "## value iteration\n",
        "env = Environment(punish = -1, actionPrice=-0.01, goalReward=1000)\n",
        "agent = Agent(0, env, 0.8, 0.1, False)\n",
        "agent.visualize_policy()\n",
        "states,actions,rewards = agent.take_action()\n",
        "agent.visualize_policy()\n",
        "plt.plot(np.array(states)[:,0], np.array(states)[:,1])\n",
        "# plt.axis('off')\n",
        "plt.grid()\n",
        "plt.gca().invert_yaxis()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OzIKAJP9hJ2l"
      },
      "outputs": [],
      "source": [
        "reward5 = []\n",
        "n_steps5 = []\n",
        "for i in range(10):\n",
        "    env = Environment(punish = -1, actionPrice=-0.01, goalReward=1000)\n",
        "    agent = Agent(0, env, 0.8, 0.1,False)\n",
        "    states,actions,rewards = agent.take_action()\n",
        "    reward5.append(np.sum(rewards))\n",
        "    print(i, np.sum(rewards))\n",
        "    n_steps5.append(len(states))\n",
        "    \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PU36NlcthJ2l"
      },
      "outputs": [],
      "source": [
        "print(f'''\n",
        "value iteration agent average reward = {np.mean(reward5)}\n",
        "value iteration agent average number of steps = {np.mean(n_steps5)}\n",
        "''')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cVySadPkhJ2l"
      },
      "outputs": [],
      "source": [
        ""
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "HW3 (2).ipynb",
      "provenance": []
    },
    "interpreter": {
      "hash": "83fad98a7911d3a2a55c2e5234aea09e74d0252d0d10d90172c6e09f21426bdf"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}